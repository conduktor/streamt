# streamt init - Bootstrap from Existing Infrastructure

## Overview

A command to create `stream_project.yml` and source definitions by discovering an existing Kafka/Flink environment. This eases migration to streamt for teams with existing infrastructure.

## Command

```bash
streamt init [OPTIONS]
```

## What It Discovers

| Infrastructure | What to Discover |
|----------------|------------------|
| Kafka | Topics, configs (partitions, replication, retention) |
| Schema Registry | Avro/JSON/Protobuf schemas for topics |
| Flink | Running jobs? (unclear if useful) |
| Connect | Existing connectors? |
| Gateway | Virtual topics? |

## Open Questions

### 1. Connection Configuration

How does the user provide connection details?

**Option A: CLI arguments**
```bash
streamt init \
  --kafka-bootstrap localhost:9092 \
  --schema-registry http://localhost:8081
```

**Option B: Interactive prompts**
```bash
streamt init
? Kafka bootstrap servers: localhost:9092
? Schema Registry URL (optional): http://localhost:8081
```

**Option C: Environment variables**
```bash
export KAFKA_BOOTSTRAP_SERVERS=localhost:9092
streamt init
```

**Option D: Existing config file**
```bash
# User creates minimal stream_project.yml with runtime section first
streamt init --discover-sources
```

### 2. Topic Filtering

Which topics to include?

- Exclude internal topics (`__consumer_offsets`, `_schemas`, etc.)?
- Filter by pattern (`--include "orders.*"`, `--exclude "_*"`)?
- Prompt user for each topic?

### 3. Source Naming

How to name the discovered sources?

- Use topic name directly: `orders.created.v1` → source name `orders.created.v1`
- Sanitize for YAML: `orders.created.v1` → `orders_created_v1`
- Ask user to provide prefix/pattern?

### 4. Schema Handling

When Schema Registry is available:

- Extract columns from schema → generate `columns:` section
- Infer event time columns (fields named `timestamp`, `event_time`, `created_at`)?
- Set `schema.subject` reference?

When no Schema Registry:

- Generate source with `columns: []` (empty)?
- Skip column generation entirely?

### 5. Output Structure

What files to generate?

**Option A: Single file**
```
project/
└── stream_project.yml  # Everything inline
```

**Option B: Organized directories**
```
project/
├── stream_project.yml  # Runtime config only
└── sources/
    ├── orders.yml
    ├── users.yml
    └── events.yml
```

### 6. Existing Project Handling

What if `stream_project.yml` already exists?

- Error and exit?
- Merge discovered sources into existing project?
- Overwrite with `--force`?

### 7. Dry Run

Should we support `--dry-run` to preview what would be generated?

```bash
streamt init --dry-run
Would create:
  - stream_project.yml (project config)
  - sources/orders_raw.yml (12 partitions, Avro schema)
  - sources/users.yml (6 partitions, JSON schema)
  ...
```

## Proposed Implementation

### Phase 1: Basic Discovery

```bash
streamt init \
  --kafka localhost:9092 \
  --schema-registry http://localhost:8081 \
  --project-name my-pipeline
```

Generates:
- `stream_project.yml` with runtime config
- `sources/*.yml` for each non-internal topic
- Column definitions from Schema Registry (if available)

### Phase 2: Advanced Options

```bash
streamt init \
  --include "orders.*" \
  --exclude "__*" \
  --output-dir ./my-project \
  --dry-run
```

### Phase 3: Interactive Mode

```bash
streamt init --interactive
```

Walks user through discovery with prompts and confirmations.

## Example Output

```yaml
# stream_project.yml
project:
  name: my-pipeline
  version: "1.0.0"
  description: "Generated by streamt init"

runtime:
  kafka:
    bootstrap_servers: localhost:9092
  schema_registry:
    url: http://localhost:8081
```

```yaml
# sources/orders_raw.yml
sources:
  - name: orders_raw
    topic: orders.raw.v1
    description: "Discovered from Kafka"

    schema:
      subject: orders.raw.v1-value
      format: avro

    columns:
      - name: order_id
        type: STRING
      - name: customer_id
        type: STRING
      - name: amount
        type: DECIMAL(10,2)
      - name: created_at
        type: TIMESTAMP(3)
        description: "Event time column"
```

## Decision Needed

1. Which connection option (A/B/C/D)?
2. Default topic filtering behavior?
3. Single file vs directory structure?
4. Interactive mode priority?
